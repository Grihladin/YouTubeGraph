# YouTubeGraph

> Transform YouTube videos into structured knowledge graphs through semantic transcript analysis and grouping.

## üéØ Overview

YouTubeGraph is a Python-based pipeline that processes YouTube video transcripts, applies semantic grouping, and prepares them for knowledge graph construction. It combines NLP, vector similarity search, and temporal analysis to create coherent topic clusters from video content.

### Key Features

- üé• **YouTube Transcript Extraction** - Automatic transcript fetching and punctuation restoration
- üìù **Intelligent Segmentation** - Groups transcript segments by semantic similarity
- ‚è±Ô∏è **Temporal Awareness** - Maintains narrative flow with time-decay penalties
- üîç **Vector Search** - Weaviate integration for k-NN similarity queries
- üß† **LLM Concept Extraction** - Extract key concepts from groups using GPT ‚ú® NEW
- üìä **Rich Analytics** - Visualizations and diagnostics for group quality
- üöÄ **Production Ready** - Clean architecture, type hints, comprehensive docs

## üì¶ Project Structure

```
YouTubeGraph/
‚îú‚îÄ‚îÄ src/                          # Core library code
‚îÇ   ‚îî‚îÄ‚îÄ core/                     # Main modules
‚îÇ       ‚îú‚îÄ‚îÄ transcript_models.py  # Data models
‚îÇ       ‚îú‚îÄ‚îÄ punctuation_worker.py # Transcript fetching & cleaning
‚îÇ       ‚îú‚îÄ‚îÄ weaviate_uploader.py  # Weaviate integration
‚îÇ       ‚îú‚îÄ‚îÄ segment_grouper.py    # Semantic grouping algorithm
‚îÇ       ‚îú‚îÄ‚îÄ concept_models.py     # ‚ú® Concept data models
‚îÇ       ‚îú‚îÄ‚îÄ concept_extractor.py  # ‚ú® LLM-powered extraction
‚îÇ       ‚îî‚îÄ‚îÄ concept_uploader.py   # ‚ú® Concept storage
‚îÇ
‚îú‚îÄ‚îÄ scripts/                      # Executable scripts  
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py              # End-to-end processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ test_groups_quality.py   # Main quality validation (‚≠ê primary test)
‚îÇ   ‚îú‚îÄ‚îÄ visualize_groups.py      # Analytics & visualization
‚îÇ   ‚îú‚îÄ‚îÄ diagnose_embeddings.py   # Embedding quality diagnostics
‚îÇ   ‚îú‚îÄ‚îÄ init_concept_schema.py   # ‚ú® Initialize concept schema
‚îÇ   ‚îú‚îÄ‚îÄ extract_concepts.py      # ‚ú® Extract concepts from groups
‚îÇ   ‚îî‚îÄ‚îÄ query_concepts.py        # ‚ú® Query and analyze concepts
‚îÇ
‚îú‚îÄ‚îÄ docs/                         # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ GROUP_SEGMENTS.md         # Grouping strategy details
‚îÇ   ‚îú‚îÄ‚îÄ README_GROUPING.md        # Grouping module guide
‚îÇ   ‚îú‚îÄ‚îÄ IMPLEMENTATION_SUMMARY.md # Implementation overview
‚îÇ   ‚îú‚îÄ‚îÄ CONCEPT_SCHEMA.md         # ‚ú® Concept schema design
‚îÇ   ‚îú‚îÄ‚îÄ PHASE1_IMPLEMENTATION.md  # ‚ú® Phase 1 guide
‚îÇ   ‚îî‚îÄ‚îÄ PHASE1_SUMMARY.md         # ‚ú® Phase 1 summary
‚îÇ
‚îú‚îÄ‚îÄ output/                       # Generated files
‚îÇ   ‚îú‚îÄ‚îÄ transcripts/             # Processed transcripts
‚îÇ   ‚îî‚îÄ‚îÄ groups/                  # Grouped segments (JSON)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ .env                         # Environment variables (not in repo)
```

## üöÄ Quick Start

> üí° **New user?** Check out [QUICKSTART.md](QUICKSTART.md) for a 5-minute getting started guide!

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/Grihladin/YouTubeGraph.git
cd YouTubeGraph

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

Create a `.env` file in the project root:

```bash
# Weaviate Cloud Configuration
WEAVIATE_URL=https://your-instance.weaviate.network
WEAVIATE_API_KEY=your-weaviate-api-key

# OpenAI API (for embeddings via Weaviate)
OPENAI_API_KEY=your-openai-api-key
```

### 3. Basic Usage

#### Option A: Full Pipeline (Recommended) üöÄ

Process everything automatically: fetch ‚Üí punctuate ‚Üí upload ‚Üí group

```python
import sys
sys.path.insert(0, 'src')
sys.path.insert(0, 'scripts')

from pipeline import YouTubeToWeaviatePipeline

# Initialize pipeline with grouping enabled
pipeline = YouTubeToWeaviatePipeline(
    enable_grouping=True,  # Automatically group segments
    grouping_params={
        "k_neighbors": 8,
        "neighbor_threshold": 0.80,
        "adjacent_threshold": 0.70,
        "max_group_words": 700,
    }
)

# Process a video end-to-end
result = pipeline.process_video("https://www.youtube.com/watch?v=VIDEO_ID")

print(f"‚úÖ Segments: {result['segment_count']}")
print(f"‚úÖ Groups: {result['group_count']}")

pipeline.close()
```

#### Option B: Step-by-Step Processing

If you need more control over each step:

```python
import sys
sys.path.insert(0, 'src')

from core import PunctuationWorker, TranscriptJob, WeaviateUploader, SegmentGrouper

# Step 1: Fetch and process transcript
worker = PunctuationWorker()
job = TranscriptJob(
    youtube_url="https://www.youtube.com/watch?v=VIDEO_ID",
    output_dir="output/transcripts"
)
result = worker(job)

# Step 2: Upload to Weaviate
uploader = WeaviateUploader()
uploader.upload_segments(result.segments)

# Step 3: Group segments
grouper = SegmentGrouper()
groups = grouper.group_video(result.video_id)

# Export results
from pathlib import Path
grouper.export_groups_to_json(groups, Path("output/groups/my_video.json"))

# Cleanup
uploader.close()
grouper.close()
```

### 4. Using Scripts

#### Full Pipeline (Fetch + Upload + Group)

```bash
# Process video end-to-end: fetch ‚Üí punctuate ‚Üí upload ‚Üí group
python scripts/pipeline.py
```

This automatically:
- ‚úÖ Fetches YouTube transcript
- ‚úÖ Restores punctuation
- ‚úÖ Uploads segments to Weaviate
- ‚úÖ Groups segments semantically
- ‚úÖ Saves groups to `output/groups/`

#### Test Grouping Quality ‚≠ê

```bash
# Test grouping quality (comprehensive analysis)
python scripts/test_groups_quality.py VIDEO_ID

# Test multiple videos
python scripts/test_groups_quality.py VIDEO_ID1 VIDEO_ID2 VIDEO_ID3
```

**This is your main quality validation tool!** It provides:
- Quality score (0-4 scale)
- Cohesion metrics
- Timeline visualization
- Sample group inspection

#### Analysis & Diagnostics

```bash
# Visualize existing groups
python scripts/visualize_groups.py

# Diagnose low quality issues
python scripts/diagnose_embeddings.py VIDEO_ID
```

#### ‚ú® NEW: Concept Extraction (Phase 1)

```bash
# Initialize concept schema (one-time)
python scripts/init_concept_schema.py

# Extract concepts from groups
python scripts/extract_concepts.py VIDEO_ID
python scripts/extract_concepts.py --all  # Process all videos

# Query and analyze concepts
python scripts/query_concepts.py VIDEO_ID
python scripts/query_concepts.py --search "machine learning"
python scripts/query_concepts.py --quality VIDEO_ID
```

See [docs/PHASE1_IMPLEMENTATION.md](docs/PHASE1_IMPLEMENTATION.md) for complete guide.

## üß† How It Works

### Complete Pipeline

```
YouTube URL 
    ‚Üì
Fetch Transcript
    ‚Üì
Restore Punctuation
    ‚Üì
Segment (150-300 words)
    ‚Üì
Upload to Weaviate (with embeddings)
    ‚Üì
Semantic Grouping (400-800 words)
    ‚Üì
Topic Groups (JSON output)
```

### 1. Transcript Processing

```
YouTube URL ‚Üí Fetch Raw Transcript ‚Üí Restore Punctuation ‚Üí Segment by Sentences
```

- Fetches auto-generated or manual transcripts
- Restores punctuation using AI (deepmultilingualpunctuation)
- Chunks into 150-300 word segments with timestamps

### 2. Semantic Grouping

```
Segments ‚Üí Vector Embeddings ‚Üí k-NN Graph ‚Üí Boundary Detection ‚Üí Topic Groups
```

**Core Algorithm:**
1. **Build k-NN Neighborhoods** - Find similar segments via Weaviate vector search
2. **Apply Temporal Decay** - Penalize distant segments: `sim_eff = sim * exp(-Œît / œÑ)`
3. **Detect Boundaries** - Identify topic shifts via cohesion dips
4. **Greedy Growth** - Form groups with word count guardrails (400-800 words)
5. **Post-Merge** - Combine highly similar adjacent groups

See [docs/GROUP_SEGMENTS.md](docs/GROUP_SEGMENTS.md) for detailed strategy.

### 3. Output Format

Groups are exported as JSON:

```json
{
  "video_id": "VIDEO_ID",
  "num_groups": 15,
  "groups": [
    {
      "group_id": 0,
      "start_time": 120.5,
      "end_time": 245.8,
      "num_segments": 3,
      "total_words": 650,
      "avg_cohesion": 0.82,
      "text": "Combined text of all segments...",
      "segments": [...]
    }
  ]
}
```

## üìä Quality Metrics

Good grouping results show:

- ‚úÖ **Average cohesion ‚â• 0.70** - Segments within groups are semantically similar
- ‚úÖ **Word counts 400-800** - Optimal for LLM processing
- ‚úÖ **‚â•80% coverage** - Most segments assigned to groups
- ‚úÖ **Reasonable group count** - Typically 20-30 groups per hour of video

## üîß Configuration

### Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `k_neighbors` | 8 | Number of nearest neighbors to fetch |
| `neighbor_threshold` | 0.75 | Minimum similarity to keep a neighbor |
| `adjacent_threshold` | 0.70 | Minimum similarity to join adjacent segments |
| `temporal_tau` | 150s | Temporal decay constant (higher = slower decay) |
| `max_group_words` | 700 | Maximum words per group |
| `min_group_segments` | 2 | Minimum segments per group |
| `merge_centroid_threshold` | 0.85 | Similarity threshold for post-merge |

### Tuning Guide

**If groups are too fragmented:**
- ‚Üì Lower `adjacent_threshold` (e.g., 0.60)
- ‚Üë Increase `temporal_tau` (e.g., 200)

**If groups are too large:**
- ‚Üë Raise `adjacent_threshold` (e.g., 0.75)
- ‚Üì Decrease `max_group_words` (e.g., 600)

**If cohesion is too low:**
- ‚Üë Raise both thresholds
- Run `diagnose_embeddings.py` for data-specific recommendations

## üéì Best Practices

### Recommended Content Types

‚úÖ **Works Great:**
- Educational tutorials
- Technical presentations
- Single-speaker lectures
- Gaming commentary
- Product reviews

‚ö†Ô∏è **Challenging:**
- Multi-speaker interviews (topic switching)
- Debates (adversarial content)
- Music videos (minimal dialogue)

### Workflow

#### Quick Start (Full Pipeline)

1. **Set up environment** - Create `.env` with API keys
2. **Run pipeline** - `python scripts/pipeline.py` (processes and groups automatically)
3. **Visualize** - `python scripts/visualize_groups.py` to check quality
4. **Iterate** - Adjust `grouping_params` in pipeline if needed

#### Advanced (Manual Control)

1. **Start Small** - Test with 1-2 videos first
2. **Diagnose** - Run `python scripts/diagnose_embeddings.py VIDEO_ID` to understand similarity
3. **Tune** - Adjust hyperparameters based on diagnostics
4. **Validate** - Use `python scripts/visualize_groups.py` to check quality
5. **Scale** - Batch process with `python scripts/run_grouping.py`

## üìö Documentation

- **[GROUP_SEGMENTS.md](docs/GROUP_SEGMENTS.md)** - Detailed grouping strategy and algorithm
- **[README_GROUPING.md](docs/README_GROUPING.md)** - User guide for segment grouping
- **[IMPLEMENTATION_SUMMARY.md](docs/IMPLEMENTATION_SUMMARY.md)** - Implementation details and next steps

## üéØ Current Status

### ‚úÖ Completed
- [x] YouTube transcript fetching
- [x] Punctuation restoration
- [x] Semantic segmentation (150-300 words)
- [x] Weaviate upload with embeddings
- [x] Semantic grouping (400-800 words)
- [x] Temporal coherence preservation
- [x] Automated end-to-end pipeline
- [x] Quality analytics & visualization

### üöß In Progress
- [ ] LLM-based concept extraction from groups
- [ ] Cue-phrase detection and cross-references

## üîÆ Future Enhancements

- [ ] Cross-video concept linking
- [ ] Neo4j knowledge graph integration
- [ ] Hierarchical grouping (groups ‚Üí topics ‚Üí themes)
- [ ] Web UI for interactive exploration
- [ ] Multi-language support
- [ ] Real-time streaming support

## ü§ù Contributing

This is a research/experimental project. Contributions welcome!

## üìù License

MIT License - See LICENSE file for details

## üôè Acknowledgments

- **Weaviate** - Vector database
- **OpenAI** - Text embeddings (text-embedding-ada-002)
- **deepmultilingualpunctuation** - Punctuation restoration
- **youtube-transcript-api** - Transcript fetching

---

**Status:** ‚úÖ Transcript processing and grouping complete. Ready for idea extraction phase.

**Author:** [Grihladin](https://github.com/Grihladin)

**Version:** 0.1.0
